{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "34bhVyDQJzdZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "GRPZtUZMJ5jO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MiAbzEvJ7XG",
        "outputId": "9b05bb0a-041d-4619-a1de-80a9e1047063"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2IPicpfJ90R",
        "outputId": "dc3d8250-6ee9-4634-dd28-c44b55d730b8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxWUIXgfKBZL",
        "outputId": "979006a1-353e-4ccb-dc13-7abb74dd19dc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBW01xJZKIA_",
        "outputId": "7256d54b-0958-41f9-8355-96c6f856e70e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "x6i83orxKJpc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3vvknNEKK1O",
        "outputId": "5259d05a-f9b9-4795-908c-63f93e259129"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "H9fMvgxTKMGu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqAtcTcHKYf0",
        "outputId": "230f985c-bcbc-48bb-ae3a-fc0736834380"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1_DAu24KZfQ",
        "outputId": "87ab30fb-6672-47ae-e1f2-492779fe3be5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "7e1xF2SHKai4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFo9zaABKboC",
        "outputId": "8c96affe-6773-4c76-f84c-9a7bd69a8d11"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "yjkhhsg8KdUr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eNk_oCZKeVN",
        "outputId": "33a1d450-5b47-4a53-c664-d6ba7416a63a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "ampYEIJbKfRc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY02SHq5Kg8N",
        "outputId": "fddc5c2f-024d-4c99-89fc-bc6eb437d5a6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-vuD-Y8KiiU",
        "outputId": "44749970-e11a-4889-8bed-a691bab6e871"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "ZDB0NKjXKkOt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymv6ORXBKl45",
        "outputId": "e56cb49f-5044-4f6a-8f71-ac5deb544b11"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "5gLCDZhgKmxE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLYD6upOKn0L",
        "outputId": "00703872-30a3-4fad-f4ce-0f0f69953c6c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whJfS8ybKpKR",
        "outputId": "d8b7d4fa-4e8e-4447-99c1-30675c5683c0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "gPCsPLaMKquG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "0hOYglzOKsPO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "aqnyddhqKtJp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rZTNV7fKuS0",
        "outputId": "d1831e50-3541-48b0-89b4-f51ae062a69a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAEOav5mKvZa",
        "outputId": "e0ad448e-177a-49c4-f55a-e793f9cf5f6a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "A9HgkeJ9KwWw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4JHWjMIKxp5",
        "outputId": "dbde8c0c-5a0c-4d59-fde0-ee4c3b5cea31"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([52,  4, 37, 47, 31, 44, 19, 11, 45, 40, 44, 40, 47, 39, 49, 20, 65,\n",
              "        7, 29, 33, 56, 35, 56, 11,  8, 42, 32, 24, 36, 11, 46, 27, 28, 61,\n",
              "       47, 62, 15, 33, 51, 33, 51, 60, 20, 34, 29, 53, 56, 51, 24, 43, 43,\n",
              "        4, 35, 24, 55, 31, 28, 19,  6, 42, 17, 29, 29,  9, 52, 16, 24,  9,\n",
              "       52, 27, 24, 14,  4,  6, 58,  4, 60, 53, 35, 17, 29, 35, 37, 43, 13,\n",
              "       10, 28, 65,  9, 10, 27, 20, 62, 46, 12, 30, 34, 61, 16, 23])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx_ESOV6Kyzd",
        "outputId": "6e017cd9-bfb3-4ddc-f760-4cd7c7b49e34"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b' true; for truth is truth\\nTo the end of reckoning.\\n\\nDUKE VINCENTIO:\\nAway with her! Poor soul,\\nShe sp'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"m$XhReF:faeahZjGz,PTqVq:-cSKW:gNOvhwBTlTluGUPnqlKdd$VKpROF'cDPP.mCK.mNKA$'s$unVDPVXd?3Oz.3NGwg;QUvCJ\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "5xOH0vihK0JR"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4ETufy4K1ap",
        "outputId": "af20895e-221e-40df-c0f0-0e065e18f18e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.188618, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2ED3fzdK2Y9",
        "outputId": "5e0b67c9-c9d7-46ec-db27-a4f6fdcce238"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.931625"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "5zJvEccaK3mo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "SYdJFoGXK4qG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "KRYgfyQtK58U"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWajMu-xK62k",
        "outputId": "062d9261-ba37-4090-d30c-b3276166ca52"
      },
      "execution_count": 45,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 600s 3s/step - loss: 2.6770\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 589s 3s/step - loss: 1.9655\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 589s 3s/step - loss: 1.6905\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 589s 3s/step - loss: 1.5343\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 590s 3s/step - loss: 1.4384\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 590s 3s/step - loss: 1.3721\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 588s 3s/step - loss: 1.3210\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 594s 3s/step - loss: 1.2755\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 592s 3s/step - loss: 1.2346\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 593s 3s/step - loss: 1.1947\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 592s 3s/step - loss: 1.1530\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 592s 3s/step - loss: 1.1106\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 595s 3s/step - loss: 1.0659\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 597s 3s/step - loss: 1.0197\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 597s 3s/step - loss: 0.9701\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 595s 3s/step - loss: 0.9188\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 596s 3s/step - loss: 0.8664\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 597s 3s/step - loss: 0.8140\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 598s 3s/step - loss: 0.7634\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 597s 3s/step - loss: 0.7174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "hzlhdLmKK7xP"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "JQDbrNdkK9eB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEVxzUN5K-tu",
        "outputId": "261c7bb7-d390-4e6d-91ab-f89f502a918c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "The worthy Mantua usurp'd,\n",
            "Come hither to visit the fairest circums\n",
            "of but best upon thy town, and other mints,\n",
            "And raphed villain, redemption himself\n",
            "To damnath my daughter my prophet wall:\n",
            "Signior Hortensio, what's it is: and who\n",
            "shall in nothing drubs and trust my happiness and patience.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Go, as it were, my lord.\n",
            "\n",
            "PRINCE EDWARD:\n",
            "And to beat back to us or true? is free of young\n",
            "Prove a specious fiend? what's a man of all and heir?\n",
            "Ol hollow Marcius.\n",
            "\n",
            "MENENIUS:\n",
            "You are abused!\n",
            "\n",
            "SEBASTIAN:\n",
            "By my tongue in your own well-ede-ting in hit\n",
            "with trember fellow Isabel. I am sell-advised:\n",
            "All truth, let me be put in beauty's valour,\n",
            "In private is thy met as dance\n",
            "there words: you know the care we must,\n",
            "And turn in usural granted mining,\n",
            "Which craves in a little be a practise.\n",
            "\n",
            "HORTENSIO:\n",
            "I say she'll say, he will repouse a lover may,\n",
            "To civil what is the which you will wish them eyes,\n",
            "Twriving it out of battle was wont to break,\n",
            "Sit in his looking on a piece of birth,\n",
            "The pres \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.3649697303771973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAldsZjfK_3m",
        "outputId": "5d0ed74b-d3a4-49f4-8ba3-5b2115c9d88f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThe true knights, of such damned swords,\\nTo with the last, what of that dead,\\nTo win your friend their wills for thy effection\\nWith all the Ledis o'er his deign. The king's noticies.\\nMadam, I'l banish you at the thirm and, as thou rags' taxking\\nThese a britten and command; and I think,\\nTo do beto my dangerous sparcele.\\n\\nKING LEWIS XI:\\nThen, good my friend, what strange man?\\nWhy droys no joy? use and a subject see,\\nBy this unmather, I will give them mo\\ndefirely and statutes and wives Kate a drop;\\nAppear in earnest hath no shalleign as every power,\\nAnd bid my sweet oppose to question; but your glad and\\nthat they are undo endury night.\\n\\nJOHN OF GAUNT:\\nWhat, not a word? where ragest with a bird?\\n\\nLADY CAPULET:\\nMy lord,\\nAs foread men he knows Isabel's.\\n\\nANGELO:\\nAnd so I had thought it hated and thank,\\nTo strike at Pomfret bloodish father!\\nLet him be gone, to't boltly; and did you life;\\nThou wouldst he did withou? pray you, bid me leave\\nyou but as you sile no less than woe,\\nNo man will lade\"\n",
            " b\"ROMEO:\\nTaught me, Marcius, widow, in that ordain.\\nHold, tell me, Signor God wonder! why\\nwhereupon is held at Coriolanu?\\n\\nServant:\\nAn hour is churchmen, and I have discharged,\\nSince I am cravely admity: a foolish in\\nthe vaulty kill.\\n\\nANGELO:\\nThe chook, what then?\\n\\nMegant:\\nAh, cousin, I do repent?\\nShe wail the present prefern turns. Here dead,\\nThe rather fall in blood to us withal,\\nIn answer fancy and no wife as is\\nI will see hit you have sworn and here withal.\\n\\nHORTENSIO:\\nHer father will choose I am Lucentio.\\n\\nTRANIO:\\nNow, good Catesby, go, God and go went,\\nWert mine own false parents,\\nWhich we fierd of lick, their nobless reasons faces,\\nNot doubt in a suit of all to give:\\nWhy envy honour father men in this out,\\nAnd though they will content you, go to your choler:\\nthink you we laughed at.\\n\\nAUTOLYCUS:\\nWhy, what's a mercian's monster torment?\\n\\nCLARENCE:\\nAll things in't, and determined of his;\\nAnd with no bigger thousand minds?\\n\\nFirst Senator:\\nBe caster, you will be bolder gold.\\n\\nKING EDWARD IV:\"\n",
            " b\"ROMEO:\\nGo, hold thy weapon, thought! and from the good gods,\\nThough thy report so noble have I heard his youth\\nTo lay blood whit the duke and unrepant. Tell him,\\n'fore your great parts offend my in myself,\\nTill I be brought to bodiey constant,\\nTo you, if you cannot, best you stoop,\\nThese followers to do move myself\\nconsidered of his brother's pale: I do beseech\\nyou, his apparent-soed is the king.\\n\\nGEENT:\\nMy dread crown is slain; and I took moved his left.\\n\\nRICHARD:\\nYour grace, is he not like a god.\\n\\nKING HENRY VI:\\nAnd stitual doth lie all dif mine thought\\nTo is the action of itself nor at\\nwhich well know how.\\n\\nDUCHESS OF YORK:\\nI'll kill the fatal own fortune of my kindness;\\nI live with these my rights and goods become a lady.\\nO, if I did wish it should be--\\nHow comes your head and kind compassion in.\\n\\nKING EDWARD IV:\\nWhy, Clarence, come! you were I but began.\\nGive me the table, and now, venition\\nIf you change your enemy: his summer life\\nOften blessed man! O, farewell, my sired, now thou know\"\n",
            " b\"ROMEO:\\nWhat was your will?\\n\\nSecond Messenger:\\nAs cruel sleeps and be brist in their eyes\\nOf eye-dounted arms.\\n\\nJULIET:\\nAt England's mother, rag's my wrongs:\\nCome, Kate of Linen our lovely fatal-greeted\\nlaning one asheed and unsatiate\\nSince I said hath received nightingale. But, how do your\\nplots have known to nought to murder, prove a spirit\\nAnd make consent to have a littles of thine own\\nconsun.\\nWill you go to ten thousand lights of drystiness;\\nIn being rather wise! come, must with!\\nWhich drinks I am to be kind corners of the King of\\nAnswer to be wish'd, and he should bew,\\nAwaked the herd to stead by men to be\\nMult perform; and, if we will forswear it.\\n\\nYORK:\\nWill you not have seen to be dangerous,\\nThat ornament to sup?\\n\\nPOLIXENES:\\nTo shrink? do you know the news;\\nI will never go with us: as thou art relends!\\nNow, and you go me wrong,\\nMore heavy wife that I am known;\\nAnd, with thy love till now a mile be many,\\nMyself disguisite. I would kiss to fear and how\\nMust hear you know he visit her:\\n\"\n",
            " b\"ROMEO:\\nGentlemen, for prunish, Paulina,\\nDo you come to you; for I mean to teach,\\n'tis butter to my love. A single blast the night\\nThat know'st thou forth my sensual place,\\nTell me of your gods, like a craven unworal,\\nLest thou remember me away.\\n\\nKING HENRY VI:\\nFrom wont-to comfort no withal tranio.\\n\\nHORTENSIO:\\nMistress Bianca, boldly peace\\nEver your father was begrian to purchase.\\nDeath and her father is confessor,\\nAnd basing sun Edward, and England\\nsnew your excellents, I will not out of down below.\\n\\nSLY:\\nHave you been admiaties, he were no less\\nThan these choke shall not she will have a bower incoildrry\\nWhilst whose spiriting souls of majesty\\nThan fear at Barnardine.\\n\\nClown:\\nNay, if I be required.\\n\\nMERCUTIO:\\nNay, stay, ere now, nought a mind to hide his knowleds.\\n\\nProvost:\\nHers, are you see, sir! have you thus. Sirrah Britagons,\\nBut thou art not bed, and he signified.\\nYou have no cut out and seemeth good at a\\ngive but one, a brawl and stopar speed\\nTo plant their own judgment-painted alms:\\n\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.0455167293548584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "id": "WNDpkYLKLBXI",
        "outputId": "e88baec2-c843-460d-a6a2-a4257581478d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fd2fd732460>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "id": "alX9-ajFLCtt",
        "outputId": "3e1753ec-4a8d-4888-aae1-43923ff8f41f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Nay, rather, well-see's bast end.\n",
            "\n",
            "AUFIDIUS:\n",
            "Why, what's this?\n",
            "\n",
            "PAdE:\n",
            "How long a throw of it.\n",
            "\n",
            "CORI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "BvPFyCsYLDqB"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "N5ytcOo5LFsF"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "ZKAmX955LGsE"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "id": "AQKQF3_7LHyO",
        "outputId": "e7866dc9-d286-4548-9e21-7a34ad34c00c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 635s 4s/step - loss: 2.7385\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd2fb002b80>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "id": "_PwP0mfnLJI3",
        "outputId": "593e5768-935f-4078-d4bd-a342dbea2bec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1709\n",
            "Epoch 1 Batch 50 Loss 2.0537\n",
            "Epoch 1 Batch 100 Loss 1.9934\n",
            "Epoch 1 Batch 150 Loss 1.8647\n",
            "\n",
            "Epoch 1 Loss: 2.0094\n",
            "Time taken for 1 epoch 628.60 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8483\n",
            "Epoch 2 Batch 50 Loss 1.7714\n",
            "Epoch 2 Batch 100 Loss 1.6666\n",
            "Epoch 2 Batch 150 Loss 1.6097\n",
            "\n",
            "Epoch 2 Loss: 1.7265\n",
            "Time taken for 1 epoch 632.28 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6243\n",
            "Epoch 3 Batch 50 Loss 1.6249\n",
            "Epoch 3 Batch 100 Loss 1.5861\n",
            "Epoch 3 Batch 150 Loss 1.5130\n",
            "\n",
            "Epoch 3 Loss: 1.5629\n",
            "Time taken for 1 epoch 631.34 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4873\n",
            "Epoch 4 Batch 50 Loss 1.4776\n",
            "Epoch 4 Batch 100 Loss 1.4628\n",
            "Epoch 4 Batch 150 Loss 1.4308\n",
            "\n",
            "Epoch 4 Loss: 1.4617\n",
            "Time taken for 1 epoch 632.48 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3723\n",
            "Epoch 5 Batch 50 Loss 1.3717\n",
            "Epoch 5 Batch 100 Loss 1.4229\n",
            "Epoch 5 Batch 150 Loss 1.3501\n",
            "\n",
            "Epoch 5 Loss: 1.3929\n",
            "Time taken for 1 epoch 631.81 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3697\n",
            "Epoch 6 Batch 50 Loss 1.3777\n",
            "Epoch 6 Batch 100 Loss 1.2911\n",
            "Epoch 6 Batch 150 Loss 1.3446\n",
            "\n",
            "Epoch 6 Loss: 1.3404\n",
            "Time taken for 1 epoch 630.45 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2919\n",
            "Epoch 7 Batch 50 Loss 1.2857\n",
            "Epoch 7 Batch 100 Loss 1.2698\n",
            "Epoch 7 Batch 150 Loss 1.2791\n",
            "\n",
            "Epoch 7 Loss: 1.2960\n",
            "Time taken for 1 epoch 630.83 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2336\n",
            "Epoch 8 Batch 50 Loss 1.2716\n",
            "Epoch 8 Batch 100 Loss 1.2647\n",
            "Epoch 8 Batch 150 Loss 1.2768\n",
            "\n",
            "Epoch 8 Loss: 1.2550\n",
            "Time taken for 1 epoch 629.63 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1985\n",
            "Epoch 9 Batch 50 Loss 1.2226\n",
            "Epoch 9 Batch 100 Loss 1.2156\n",
            "Epoch 9 Batch 150 Loss 1.2051\n",
            "\n",
            "Epoch 9 Loss: 1.2167\n",
            "Time taken for 1 epoch 630.64 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1514\n",
            "Epoch 10 Batch 50 Loss 1.1514\n",
            "Epoch 10 Batch 100 Loss 1.1927\n",
            "Epoch 10 Batch 150 Loss 1.2287\n",
            "\n",
            "Epoch 10 Loss: 1.1783\n",
            "Time taken for 1 epoch 631.97 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}