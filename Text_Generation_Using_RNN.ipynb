{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "34bhVyDQJzdZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GRPZtUZMJ5jO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5756a96-2512-47de-ee0d-0189cc89ef23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MiAbzEvJ7XG",
        "outputId": "4f971701-c47f-4f8f-fc4d-b2660a480cd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2IPicpfJ90R",
        "outputId": "4b85fa12-bf7b-468c-d510-7ab17fd0f376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxWUIXgfKBZL",
        "outputId": "1124ba2c-0724-422c-cd06-799f289b64a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBW01xJZKIA_",
        "outputId": "077f79d5-1ad6-4cf7-f4e2-24d2a3769b7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x6i83orxKJpc"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3vvknNEKK1O",
        "outputId": "3d50502b-46d7-4a94-af3b-fb0adf76afbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "H9fMvgxTKMGu"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqAtcTcHKYf0",
        "outputId": "eccdb044-e4c3-4bbc-8b9a-2de01670d1f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1_DAu24KZfQ",
        "outputId": "d9f4ed60-44eb-43b5-8c75-569c0e2cd791"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7e1xF2SHKai4"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFo9zaABKboC",
        "outputId": "83ff1ee8-29ba-4de8-bc2b-46a963299bdb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yjkhhsg8KdUr"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eNk_oCZKeVN",
        "outputId": "d9c5f0c9-3d44-454e-d98d-8e9383ff8c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ampYEIJbKfRc"
      },
      "outputs": [],
      "source": [
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY02SHq5Kg8N",
        "outputId": "ace7dcb5-a877-4bbc-90d9-dae63f4294a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-vuD-Y8KiiU",
        "outputId": "93f8cbc5-f74f-4b63-9751-fec32ef31c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZDB0NKjXKkOt"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymv6ORXBKl45",
        "outputId": "11d6f26f-e48a-4e79-fd8e-2ab3d4a9ba99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5gLCDZhgKmxE"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLYD6upOKn0L",
        "outputId": "f09c8d0f-38d3-4be6-e0c0-c6507a41f3d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whJfS8ybKpKR",
        "outputId": "b8f28e03-2bd5-4b11-e1b4-087033babf27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gPCsPLaMKquG"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0hOYglzOKsPO"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "aqnyddhqKtJp"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rZTNV7fKuS0",
        "outputId": "0be927e6-473e-4bf7-bae8-d0b8e330e0b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAEOav5mKvZa",
        "outputId": "1fee0efd-0789-42c1-82c6-283406b3424a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "A9HgkeJ9KwWw"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4JHWjMIKxp5",
        "outputId": "f785cd9f-1599-43aa-e8ff-5badfda104c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([46,  9, 15,  6, 25, 48, 53, 59, 35, 43, 39, 31, 36, 23, 61,  7, 42,\n",
              "       48, 44, 21, 54, 63, 44, 42, 57, 14, 12, 24, 54, 23, 61, 39, 18, 11,\n",
              "       29, 59, 26, 33, 64,  8,  2, 57,  1, 16, 61, 34, 22, 16, 62, 16, 64,\n",
              "       22, 60, 24, 18,  3, 41, 11,  6, 30, 43, 53, 39, 53, 64, 52, 54, 10,\n",
              "        7, 59,  2, 37,  0, 13, 22, 40, 39, 40, 52, 55,  9, 25, 32, 42, 30,\n",
              "       15, 17, 24, 43,  2, 52, 42, 33, 14, 62,  1, 57, 51, 45, 31])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx_ESOV6Kyzd",
        "outputId": "17b3dd7f-c8eb-442d-8cfb-94eaf29d80e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'which way to order these affairs\\nThus thrust disorderly into my hands,\\nNever believe me. Both are my'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"g.B'LintVdZRWJv,cieHoxecrA;KoJvZE:PtMTy- r\\nCvUICwCyIuKE!b:'QdnZnymo3,t X[UNK]?IaZamp.LScQBDKd mcTAw\\nrlfR\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5xOH0vihK0JR"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4ETufy4K1ap",
        "outputId": "6c1d5a30-85ee-48fc-d8ec-47453113faa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.190334, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "N2ED3fzdK2Y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2c8f89d-839e-41e2-af02-d4b5b9f83c9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.04484"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "12A_43PyXDQ3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "TFyu62JFXD4g"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "a6GWRXtmXFyC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTXCK1BlXHvc",
        "outputId": "418dae62-9bd3-4597-ad13-a58499c0fbef"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 15s 50ms/step - loss: 2.7179\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.9891\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.7011\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.5413\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.4439\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.3770\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.3250\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.2813\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.2401\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2008\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.1608\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.1198\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.0762\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.0294\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.9806\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.9303\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.8768\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.8248\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.7743\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.7256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "1V5o4HU2XJO_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "3-ncVb-zXLvG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY2R5kWsXNrm",
        "outputId": "bc937d0b-9cf9-45bd-f8dc-886d5435b539"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "She speak again; but yet I will then be given me.\n",
            "Here is a gentleman in blood was never in a natural terst\n",
            "For instruction made, longing, as I say,\n",
            "sir, I wonder here I thank you, for your honour can think\n",
            "Me well to Friar Lauce your husband and his fashion;\n",
            "For she should say 'tis since the benefit of this rage,\n",
            "In thy sip' boved on thy brother, or I\n",
            "may, then, 'tis sheep, which quietness of the king.\n",
            "Have you a maid of il so much, by his age,\n",
            "Or mew'd the name of kings. Letter.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Go not to London, towards God gladly quaking my contracted\n",
            "Helping Bolingbroke? I saw him.\n",
            "\n",
            "LADY CAPULET:\n",
            "And thou the last?\n",
            "\n",
            "LEONTES:\n",
            "How!\n",
            "\n",
            "GREY:\n",
            "My husband and sour wrongs what you can but say\n",
            "'Shell for thee, I know no limit. Compet it withal!\n",
            "Alack, are you against a man: old gentlemen,\n",
            "For inquent with the olacle!\n",
            "Saw some saveguly, therefore let us be noted fit lineally.\n",
            "\n",
            "KING HENRY VI:\n",
            "And, by the jest? that will I deat together,\n",
            "Has clouded love him curses note upon these\n",
            "same mon \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.130950927734375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob75igXQXPVG",
        "outputId": "92b1c7a0-b957-4344-cc76-b50482ac4985"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nGon and being warm'd and froward over'd.\\nGod make me not, my lord, I'll one thee have\\nI promised your and a map or wilt.\\n\\nPETRUCHIO:\\nWomen we pray?\\n\\nPOMPEY:\\nMy lord?\\n\\nPage:\\nHe die, in false of his contempt, speeds for thee\\nI will to venge a father; and known blown out of run\\nThe riss and city Coriolanus\\nPlease him to send him so still shining steel.\\n\\nKING RICHARD II:\\nThis is good news? by the world--\\n\\nKING RICHARD III:\\nStay, O, the duke no streak grave to\\nSure the belly of a familiar resolve\\nSome peril even hours on evil; we do rejoin\\nA piece of embling witness of my suit's time.\\nTell me this day lions, I warranting the firms.\\n\\nCLAUDIO:\\nYet be the motion of yond conquerer:\\nTo you, the hour should see thy choice.\\n\\nBISHOP OF ELY:\\nMarry, sir, train, strong names and defence, whom they be a\\nkind of the duke.'\\n\\nSAMPSON:\\nDraw far forthwith cannot flatter up in\\ncharged as 'tis too. which has many doubtful gold,\\nBecause you more to sue, our garters names the heart.\\nWhere's Bona of this maid w\"\n",
            " b\"ROMEO:\\nThe lorks and lady have some word thinks me promises me\\nAnd then be falce for some that are but swear;\\nI'll grant her me; the viger and mine own\\nhonour for a second Warling marriage call'd\\nAgainst a hundred covoration.\\n\\nBIONDELLO:\\nNot a word; all in all too subsle.\\n\\nISABELLA:\\nI have.\\n\\nSEBASTIAN:\\nAn it had I boy: here it is,\\nyou then displass swallowing in the vessel-house,\\nAnd thou dost say, but see that maths as seas;\\nFor so your days to Lady Bloud Lucio, look.\\n\\nROMEO:\\nShe hangs another'd: good sir! my leave, and I'll be your\\nAnswer with her, my mistress criming,\\nWe are not seen thy will his way?\\nFarroly you yours, good counsel: but the worst is,\\nSon takes a load, and yesternity than never thus\\nNot spoil'd. By news, and am I saw spoil?\\nHave you neglected the prince, the people's violent\\nTo beer his accompass.\\n\\nCOMINIUS:\\nI thought it please your troth,--\\nAs I will not, but dares not once more grateful,\\nLift up by pains, but kill with me to-day?\\nShake too much millions, if I can purcha\"\n",
            " b\"ROMEO:\\nTend to end it all; for he the sunset book.\\nComes Marcius comes for his peece.\\n\\nNurse:\\nAna, you cannot,--\\n\\nSICINIUS:\\nGo, your brows about?\\n\\nProvost:\\nA most children; elext in her heart's good trifles. What, duty to the prince?\\n\\nKING EDWARD IV:\\nSo much it earth so hasty to the duke.\\n\\nESCALUS:\\nHail, lords, be with you, who added to me and long love here\\nAs I can front, and live with interious reggor\\nIs not answer. Let me be burnt.\\nA murdered your mislike as it\\nicquarts, we will a know.\\n\\nFLORIZEL:\\nMore than me?\\nGood monswoulds! follow me as for a sheek,\\nAnd I they see thee homely: but\\nI had rather be a precespast to be spoke to thee.\\n\\nAUTOLYCUS:\\n\\nShepherd:\\nYet, he sings\\nComes the adverning yielding of the two kings,\\nFive wither'd blass my pipes with meaner lambs with death.\\nIf thou deniest them both. Welcome to England?\\n\\nYORK:\\nOf you, my double fool, how shall I have a stone--\\nNurse: he's a desire to do in's pains it off; but, I\\nperchase, I say good son: be not\\nIn wholesome feeming: boot\"\n",
            " b\"ROMEO:\\nTell me this mother, why, what one lord?\\nThat senison'd interruptures faced Courage? How do you fill\\nthe words to't a perpetual ripens by queen,\\nAnd show deal observer in my service\\nNot befits that tenth unactide.\\n\\nSLY:\\nWe'll have no doubt: for thou art never spill my lady's sister\\nTo trust the white on him scarce the vied wield:\\nBeing but bloody blocks with swore, through'd,\\nThat song, your son of Birtains, they say,\\nScariling the hour but himself,\\nStands your short: hon so young, we turn your tongue,\\nThat continud my name I come to thee\\nThat we have wrought you that love her bestrew'd him,\\nScotling the gloves both the fault and ingrate and rich;\\nBut now the lawour ride aptaining Hereford;\\nForget flow with joy?\\nShake fit to open the wolf, our spleens too rud,\\nAnd bid her counsel-able tears be beauted\\nThe people's malmow lies:\\n\\nPETRUCHIO:\\nNay, I can counterve her dowry.\\n\\nFROTH:\\nA bear-man! Wrench up the dearest bleed\\nAnd that unwilling thus long advertion,\\nMake me alone with Bolingbro\"\n",
            " b\"ROMEO:\\nSweet Oxey you do't, so much to deny,\\nI may, no other first may not a case.\\nNow tell me, who comest thou humble luss?\\n\\nKING RICHARD III:\\nDeliver thy spirit! Sound drums' not the ground?\\nIs we devise for the way.\\nWe do believe thee! welcome! Volscol! father!\\nBut what of her to come; you know not yourself,\\nTrust to you. Mark you there.\\nGive me the moon, our starward too;\\nHave we no outser and displeased\\nWith Bolingbroke to be contemned with thy packs;\\nAnd when cunsisting I have left possess'd\\nThyself and now again before 'tis thence.\\nSpeak not, his barbering come and noble lord?\\n\\nHENRY BOLINGBROKE:\\nLove them Warwick cut my gage should entreat you.\\n\\nMARIANA:\\nIn busy carries aside, and well 'bed him too redres,\\nFor beauty and lovers' mighty spoke of four.\\nFor and for my mislike Claudio,\\nWhich she use to thy bed, and that name Norts, blessed lambs?\\nGo; company it, proud queen, to break a sequence;\\nWith true kind to the rock, that I shall fain.\\nO, I will meet him, we kill'd her unstone\\nAnd \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.147644281387329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1En1M-PXRbi",
        "outputId": "892e51e9-74f5-4e32-aa82-e5541a58ea2e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f41a3fd0f10>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR5MudHlXTke",
        "outputId": "73506c67-4e8f-4de6-e4b8-d2d0d612a54f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "God and but myself and unnaturally lies\n",
            "And more inconstant to the ground infarty haves.\n",
            "O Thom, of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "mnNoFJd7XVPH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "wZD_JxvJXXaE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "-jaql6ihXYx1"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWPnN5mqXaTy",
        "outputId": "23be0297-97a2-4b1b-8e2e-e350ba223930"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 13s 54ms/step - loss: 2.7033\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f41a3fda760>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl2eweAVXcZm",
        "outputId": "346b8ea1-6d10-446c-8acb-f902a500e867"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1729\n",
            "Epoch 1 Batch 50 Loss 2.0649\n",
            "Epoch 1 Batch 100 Loss 1.9938\n",
            "Epoch 1 Batch 150 Loss 1.8364\n",
            "\n",
            "Epoch 1 Loss: 1.9833\n",
            "Time taken for 1 epoch 10.76 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8038\n",
            "Epoch 2 Batch 50 Loss 1.7189\n",
            "Epoch 2 Batch 100 Loss 1.6290\n",
            "Epoch 2 Batch 150 Loss 1.6133\n",
            "\n",
            "Epoch 2 Loss: 1.7084\n",
            "Time taken for 1 epoch 10.03 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6226\n",
            "Epoch 3 Batch 50 Loss 1.5637\n",
            "Epoch 3 Batch 100 Loss 1.5682\n",
            "Epoch 3 Batch 150 Loss 1.4990\n",
            "\n",
            "Epoch 3 Loss: 1.5480\n",
            "Time taken for 1 epoch 10.06 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.5046\n",
            "Epoch 4 Batch 50 Loss 1.4349\n",
            "Epoch 4 Batch 100 Loss 1.4488\n",
            "Epoch 4 Batch 150 Loss 1.3947\n",
            "\n",
            "Epoch 4 Loss: 1.4497\n",
            "Time taken for 1 epoch 10.17 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4146\n",
            "Epoch 5 Batch 50 Loss 1.4121\n",
            "Epoch 5 Batch 100 Loss 1.3444\n",
            "Epoch 5 Batch 150 Loss 1.4092\n",
            "\n",
            "Epoch 5 Loss: 1.3810\n",
            "Time taken for 1 epoch 10.27 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.2845\n",
            "Epoch 6 Batch 50 Loss 1.3341\n",
            "Epoch 6 Batch 100 Loss 1.3050\n",
            "Epoch 6 Batch 150 Loss 1.3297\n",
            "\n",
            "Epoch 6 Loss: 1.3286\n",
            "Time taken for 1 epoch 10.01 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2452\n",
            "Epoch 7 Batch 50 Loss 1.3098\n",
            "Epoch 7 Batch 100 Loss 1.2574\n",
            "Epoch 7 Batch 150 Loss 1.2865\n",
            "\n",
            "Epoch 7 Loss: 1.2836\n",
            "Time taken for 1 epoch 10.00 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2191\n",
            "Epoch 8 Batch 50 Loss 1.2489\n",
            "Epoch 8 Batch 100 Loss 1.2159\n",
            "Epoch 8 Batch 150 Loss 1.2866\n",
            "\n",
            "Epoch 8 Loss: 1.2426\n",
            "Time taken for 1 epoch 9.97 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1795\n",
            "Epoch 9 Batch 50 Loss 1.2208\n",
            "Epoch 9 Batch 100 Loss 1.2096\n",
            "Epoch 9 Batch 150 Loss 1.2090\n",
            "\n",
            "Epoch 9 Loss: 1.2032\n",
            "Time taken for 1 epoch 10.02 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1385\n",
            "Epoch 10 Batch 50 Loss 1.1484\n",
            "Epoch 10 Batch 100 Loss 1.1691\n",
            "Epoch 10 Batch 150 Loss 1.1642\n",
            "\n",
            "Epoch 10 Loss: 1.1638\n",
            "Time taken for 1 epoch 10.17 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}